{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfd2e32",
   "metadata": {},
   "source": [
    "# CUDA-MODE session 4 (ch 4 + 5 of the book)\n",
    "\n",
    "Notebook by Thomas Viehmann, based on Jeremy Howard's notebook from lecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd3f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: ninja in /root/miniconda3/envs/llm/lib/python3.9/site-packages (1.11.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m/bin/bash: line 1: sudo: command not found\n",
      "/bin/bash: line 1: sudo: command not found\n",
      "/bin/bash: line 1: sudo: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install ninja\n",
    "!sudo apt update\n",
    "!sudo apt install g++-11 -y\n",
    "!sudo apt install ccache -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b545e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.cpp_extension\n",
    "import os\n",
    "os.environ['CXX'] = '/usr/lib/ccache/g++-11'\n",
    "os.environ['CC'] = '/usr/lib/ccache/gcc-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288de6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu121/test_ext/build.ninja...\n",
      "/root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module test_ext...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module test_ext...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "96.6100051 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-09-10 14:22:31 1442:1442 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-09-10 14:22:32 1442:1442 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-09-10 14:22:32 1442:1442 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       cudaLaunchKernel         6.21%      57.992ms         6.21%      57.992ms       5.799us       0.000us         0.00%       0.000us       0.000us         10000  \n",
      "rgb_to_grayscale_kernel(unsigned char*, unsigned cha...         0.00%       0.000us         0.00%       0.000us       0.000us     938.277ms       100.00%     938.277ms      93.828us         10000  \n",
      "                                  cudaDeviceSynchronize        93.79%     875.747ms        93.79%     875.747ms      87.566us       0.000us         0.00%       0.000us       0.000us         10001  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 933.739ms\n",
      "Self CUDA time total: 938.277ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# based on Jeremy's Lecture 3 notebook\n",
    "cuda_begin = r'''\n",
    "#include <torch/extension.h>\n",
    "#include <stdio.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "'''\n",
    "\n",
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void rgb_to_grayscale_kernel(unsigned char* out, unsigned char* in, int n) {\n",
    "    int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "    if (i >= n) return;\n",
    "    out[i] = 0.2989f*in[i] + 0.5870f*in[i+n] + 0.1140f*in[i+2*n];  // fix with f found by Andreas...\n",
    "}\n",
    "\n",
    "torch::Tensor rgb_to_grayscale_out(torch::Tensor output, const torch::Tensor& input) {\n",
    "    CHECK_INPUT(input);\n",
    "    int h = input.size(1);\n",
    "    int w = input.size(2);\n",
    "    TORCH_CHECK((h == output.size(0)) || (w == output.size(1)) || (output.device() == input.device())\n",
    "                || (output.scalar_type() == input.scalar_type()));\n",
    "    int threads = 256;\n",
    "    rgb_to_grayscale_kernel<<<cdiv(w*h,threads), threads>>>(\n",
    "        output.data_ptr<unsigned char>(), input.data_ptr<unsigned char>(), w*h);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "\n",
    "torch::Tensor rgb_to_grayscale(const torch::Tensor& input) {\n",
    "    CHECK_INPUT(input);\n",
    "    int h = input.size(1);\n",
    "    int w = input.size(2);\n",
    "    auto output = torch::empty({h,w}, input.options());\n",
    "    rgb_to_grayscale_out(output, input);\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "\n",
    "cpp_src = \"\"\"\n",
    "torch::Tensor rgb_to_grayscale(const torch::Tensor& input);\n",
    "torch::Tensor rgb_to_grayscale_out(torch::Tensor outpuit, const torch::Tensor& input);\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['CXX'] = '/usr/lib/ccache/g++-11'\n",
    "os.environ['CC'] = '/usr/lib/ccache/gcc-11'\n",
    "\n",
    "module = torch.utils.cpp_extension.load_inline(\n",
    "    \"test_ext\", cpp_src, cuda_src, \n",
    "    functions=['rgb_to_grayscale', 'rgb_to_grayscale_out'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "n = 2048\n",
    "t = torch.randint(0, 256, (3, n, n), dtype=torch.uint8, device=\"cuda\")\n",
    "out = module.rgb_to_grayscale(t); torch.cuda.synchronize()\n",
    "\n",
    "import time\n",
    "t0 = time.perf_counter_ns()\n",
    "for i in range(10_000): # 10000\n",
    "    module.rgb_to_grayscale_out(out, t)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.perf_counter_ns()\n",
    "\n",
    "print((t1-t0) / 10_000 / 1_000, \"µs\") \n",
    "\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    for i in range(10_000):\n",
    "        module.rgb_to_grayscale_out(out, t)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(prof.key_averages().table())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442400bd",
   "metadata": {},
   "source": [
    "# Approximate gelu as a fusion example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c8c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as per the pytorch doc, implemented manually\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1+ torch.tanh((2/torch.pi)**0.5 * (x+0.044715 * x**3)))\n",
    "\n",
    "x = torch.randn(1024, 1024, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0733f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gelu(x) - torch.nn.functional.gelu(x, approximate='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d782da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 µs ± 2.91 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "46.8 µs ± 288 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gelu(x); torch.cuda.synchronize()\n",
    "%timeit torch.nn.functional.gelu(x, approximate='tanh'); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23aa4819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-09-10 14:19:31 1442:1442 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 µs ± 32.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-09-10 14:19:34 1442:1442 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-09-10 14:19:34 1442:1442 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul        20.62%     351.065ms        50.66%     862.405ms      30.800us     963.267ms        49.80%        1.072s      38.268us         28000  \n",
      "                                       cudaLaunchKernel        60.05%        1.022s        60.05%        1.022s      18.255us     216.473ms        11.19%     216.473ms       3.866us         56000  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     647.915ms        33.50%     647.915ms      30.853us         21000  \n",
      "                                              aten::pow         6.35%     108.030ms        12.88%     219.221ms      31.317us     217.794ms        11.26%     244.856ms      34.979us          7000  \n",
      "                                      aten::result_type         0.00%      44.000us         0.00%      44.000us       0.006us       0.000us         0.00%       0.000us       0.000us          7000  \n",
      "                                               aten::to         0.00%      10.000us         0.00%      10.000us       0.001us       0.000us         0.00%       0.000us       0.000us          7000  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     217.794ms        11.26%     217.794ms      31.113us          7000  \n",
      "                                              aten::add         8.46%     143.939ms        23.75%     404.336ms      28.881us     535.707ms        27.69%     589.821ms      42.130us         14000  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     318.813ms        16.48%     318.813ms      45.545us          7000  \n",
      "                                             aten::tanh         3.98%      67.819ms        12.17%     207.213ms      29.602us     217.367ms        11.24%     244.425ms      34.918us          7000  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     217.367ms        11.24%     217.367ms      31.052us          7000  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     216.956ms        11.22%     216.956ms      30.994us          7000  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     315.475ms        16.31%     315.475ms      45.068us          7000  \n",
      "                                  cudaDeviceSynchronize         0.54%       9.174ms         0.54%       9.174ms       9.174ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.702s\n",
      "Self CUDA time total: 1.934s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile() as prof:\n",
    "    %timeit -n 1000 gelu(x)\n",
    "print(prof.key_averages().table())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a9f0de",
   "metadata": {},
   "source": [
    "## Kind of slow. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a54e12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py39_cu121/test_ext_gelu...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu121/test_ext_gelu/build.ninja...\n",
      "/root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module test_ext_gelu...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/lib/ccache/g++-11 -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=test_ext_gelu -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/TH -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/miniconda3/envs/llm/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py39_cu121/test_ext_gelu/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -ccbin /usr/lib/ccache/gcc-11 -DTORCH_EXTENSION_NAME=test_ext_gelu -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/TH -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/miniconda3/envs/llm/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' --ptxas-options=-v -std=c++17 -c /root/.cache/torch_extensions/py39_cu121/test_ext_gelu/cuda.cu -o cuda.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z14my_gelu_kernelPfS_i' for 'sm_75'\n",
      "ptxas info    : Function properties for _Z14my_gelu_kernelPfS_i\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 12 registers, 372 bytes cmem[0]\n",
      "[3/3] /usr/lib/ccache/g++-11 main.o cuda.cuda.o -shared -L/root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o test_ext_gelu.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module test_ext_gelu...\n"
     ]
    }
   ],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void my_gelu_kernel(float* out, float* inp, int n) {\n",
    "    int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "    if (i >= n) return;\n",
    "    float x = inp[i];\n",
    "    out[i] = 0.5f * x * (1.0f+ tanhf(sqrtf(2.0f/3.141592653589793f) * (x+0.044715f * (x*x*x))));\n",
    "}\n",
    "\n",
    "torch::Tensor my_gelu_out(torch::Tensor output, const torch::Tensor& inp) {\n",
    "    CHECK_INPUT(inp);\n",
    "    int n = inp.numel();\n",
    "    TORCH_CHECK((output.sizes() == inp.sizes())  || (output.device() == inp.device())\n",
    "                || (output.scalar_type() == inp.scalar_type()));\n",
    "    int threads = 256;\n",
    "    my_gelu_kernel<<<cdiv(n, threads), threads>>>(\n",
    "        output.data_ptr<float>(), inp.data_ptr<float>(), n);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "\n",
    "torch::Tensor my_gelu(const torch::Tensor& inp) {\n",
    "    CHECK_INPUT(inp);\n",
    "    auto output = torch::empty_like(inp);\n",
    "    my_gelu_out(output, inp);\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "\n",
    "cpp_src = \"\"\"\n",
    "torch::Tensor my_gelu(const torch::Tensor& inp);\n",
    "torch::Tensor my_gelu_out(torch::Tensor output, const torch::Tensor& inp);\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['CXX'] = '/usr/lib/ccache/g++-11'\n",
    "os.environ['CC'] = '/usr/lib/ccache/gcc-11'\n",
    "\n",
    "gelu_module = torch.utils.cpp_extension.load_inline(\n",
    "    \"test_ext_gelu\", cpp_src, cuda_src, \n",
    "    functions=['my_gelu', 'my_gelu_out'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec3f1b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3842e-07, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gelu_module.my_gelu(x) - gelu(x)).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3427d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.2 µs ± 540 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gelu_module.my_gelu(x); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fabbbd",
   "metadata": {},
   "source": [
    "# Empty kernel to measure launch latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6752592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/tv/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/tv/.cache/torch_extensions/py310_cu121/test_ext_empty/build.ninja...\n",
      "Building extension module test_ext_empty...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/lib/ccache/g++-11 -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=test_ext_empty -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/tv/.cache/torch_extensions/py310_cu121/test_ext_empty/main.cpp -o main.o \n",
      "[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -ccbin /usr/lib/ccache/gcc-11 -DTORCH_EXTENSION_NAME=test_ext_empty -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' --ptxas-options=-v -std=c++17 -c /home/tv/.cache/torch_extensions/py310_cu121/test_ext_empty/cuda.cu -o cuda.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15my_empty_kernelPfS_i' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15my_empty_kernelPfS_i\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 4 registers, 372 bytes cmem[0]\n",
      "[3/3] /usr/lib/ccache/g++-11 main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o test_ext_empty.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module test_ext_empty...\n"
     ]
    }
   ],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void my_empty_kernel(float* out, float* inp, int n) {\n",
    "}\n",
    "\n",
    "torch::Tensor my_empty_out(torch::Tensor output, const torch::Tensor& inp) {\n",
    "    CHECK_INPUT(inp);\n",
    "    int n = inp.numel();\n",
    "    TORCH_CHECK((output.sizes() == inp.sizes())  || (output.device() == inp.device())\n",
    "                || (output.scalar_type() == inp.scalar_type()));\n",
    "    int threads = 256;\n",
    "    my_empty_kernel<<<cdiv(n, threads), threads>>>(\n",
    "        output.data_ptr<float>(), inp.data_ptr<float>(), n);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "\n",
    "torch::Tensor my_empty(const torch::Tensor& inp) {\n",
    "    CHECK_INPUT(inp);\n",
    "    auto output = torch::empty_like(inp);\n",
    "    my_empty_out(output, inp);\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "\n",
    "cpp_src = \"\"\"\n",
    "torch::Tensor my_empty(const torch::Tensor& inp);\n",
    "torch::Tensor my_empty_out(torch::Tensor output, const torch::Tensor& inp);\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['CXX'] = '/usr/lib/ccache/g++-11'\n",
    "os.environ['CC'] = '/usr/lib/ccache/gcc-11'\n",
    "\n",
    "empty_module = torch.utils.cpp_extension.load_inline(\n",
    "    \"test_ext_empty\", cpp_src, cuda_src, \n",
    "    functions=['my_empty', 'my_empty_out'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2111adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.6 µs ± 261 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-03 18:47:47 288753:288753 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-02-03 18:47:47 288753:288753 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-03 18:47:47 288753:288753 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                        cudaLaunchKernel        80.81%      42.183ms        80.81%      42.183ms       4.218us       0.000us         0.00%       0.000us       0.000us         10000  \n",
      "    my_empty_kernel(float*, float*, int)         0.00%       0.000us         0.00%       0.000us       0.000us      30.000ms       100.00%      30.000ms       3.000us         10000  \n",
      "                   cudaDeviceSynchronize        19.19%      10.020ms        19.19%      10.020ms       1.002us       0.000us         0.00%       0.000us       0.000us         10001  \n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 52.203ms\n",
      "Self CUDA time total: 30.000ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit empty_module.my_empty_out(x, x); torch.cuda.synchronize()\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    for i in range(10_000):\n",
    "        empty_module.my_empty_out(x, x)\n",
    "        torch.cuda.synchronize()\n",
    "print(prof.key_averages().table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "836d9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5aa965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/tv/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/tv/.cache/torch_extensions/py310_cu121/test_ext_simple_matmul/build.ninja...\n",
      "Building extension module test_ext_simple_matmul...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module test_ext_simple_matmul...\n"
     ]
    }
   ],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void simple_matmul_k(float* m, float* n, float* out, int h, int w, int k) {\n",
    "    int r = blockIdx.y*blockDim.y + threadIdx.y;\n",
    "    int c = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (r>=h || c>=w) return;\n",
    "    float o = 0;\n",
    "    for (int i = 0; i<k; ++i) o += m[r*k+i] * n[i*w+c];\n",
    "    out[r*w+c] = o;\n",
    "}\n",
    "\n",
    "torch::Tensor simple_matmul(const torch::Tensor& m, const torch::Tensor& n) {\n",
    "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
    "    int h = m.size(0);\n",
    "    int w = n.size(1);\n",
    "    int k = m.size(1);\n",
    "    TORCH_CHECK(k==n.size(0), \"Size mismatch!\");\n",
    "    auto output = torch::zeros({h, w}, m.options());\n",
    "\n",
    "    dim3 tpb(16,16);\n",
    "    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
    "    simple_matmul_k<<<blocks, tpb>>>(\n",
    "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "\n",
    "cpp_src = \"\"\"\n",
    "torch::Tensor simple_matmul(const torch::Tensor& m, const torch::Tensor& n);\n",
    "\"\"\"\n",
    "\n",
    "simple_matmul_module = torch.utils.cpp_extension.load_inline(\n",
    "    \"test_ext_simple_matmul\", cpp_src, cuda_src, \n",
    "    functions=['simple_matmul'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba92c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934 µs ± 1.42 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1024, 1024, device=\"cuda\")\n",
    "b = torch.randn(1024, 1024, device=\"cuda\")\n",
    "%timeit simple_matmul_module.simple_matmul(a, b)\n",
    "\n",
    "(simple_matmul_module.simple_matmul(a, b) - a@b).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd38a25",
   "metadata": {},
   "source": [
    "## Tiled matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47b4e127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/tv/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/tv/.cache/torch_extensions/py310_cu121/test_ext_tiled_matmul/build.ninja...\n",
      "Building extension module test_ext_tiled_matmul...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/lib/ccache/g++-11 -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=test_ext_tiled_matmul -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/tv/.cache/torch_extensions/py310_cu121/test_ext_tiled_matmul/main.cpp -o main.o \n",
      "[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -ccbin /usr/lib/ccache/gcc-11 -DTORCH_EXTENSION_NAME=test_ext_tiled_matmul -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' --ptxas-options=-v -std=c++17 -c /home/tv/.cache/torch_extensions/py310_cu121/test_ext_tiled_matmul/cuda.cu -o cuda.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z19tiled_matmul_kernelPfS_S_iii' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z19tiled_matmul_kernelPfS_S_iii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 36 registers, 2048 bytes smem, 388 bytes cmem[0]\n",
      "[3/3] /usr/lib/ccache/g++-11 main.o cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o test_ext_tiled_matmul.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module test_ext_tiled_matmul...\n"
     ]
    }
   ],
   "source": [
    "cuda_src = cuda_begin + r\"\"\"\n",
    "constexpr int TILE_SIZE = 16;\n",
    "\n",
    "__global__ void tiled_matmul_kernel(float* out, float* M, float* N, int h, int w, int k) {\n",
    "  __shared__ float M_tile[TILE_SIZE][TILE_SIZE];\n",
    "  __shared__ float N_tile[TILE_SIZE][TILE_SIZE];\n",
    "  \n",
    "  // idxes into tile\n",
    "  int ir = threadIdx.y;\n",
    "  int ic = threadIdx.x;\n",
    "  \n",
    "  int r = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "  int c = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "  // note: cannot just exit if we want to do padding!\n",
    "  \n",
    "  float res = 0.0f;\n",
    "  for (int K_tileidx = 0; K_tileidx < (k + TILE_SIZE -1) / TILE_SIZE; K_tileidx++) {\n",
    "    // note how threadIdx.x is the fastes moving bit --> coalesced memory access\n",
    "    M_tile[ir][ic] = (((r < h) && (K_tileidx * TILE_SIZE + ic < k)) ? M[r * k + K_tileidx * TILE_SIZE + ic] : 0.f);\n",
    "    N_tile[ir][ic] = ((((K_tileidx * TILE_SIZE + ir) < k) && (c < w)) ? N[(K_tileidx * TILE_SIZE + ir) * w + c] : 0.f);\n",
    "    //M_tile[ir][ic] = M[r * k + K_tileidx * TILE_SIZE + ic];\n",
    "    //N_tile[ir][ic] = N[(K_tileidx * TILE_SIZE + ir) * w + c];\n",
    "    __syncthreads();\n",
    "    for (int idx = 0; idx < TILE_SIZE; idx++) {\n",
    "       res += M_tile[ir][idx] * N_tile[idx][ic];\n",
    "    }\n",
    "    __syncthreads(); // important! (why?)\n",
    "  }\n",
    "  if ((r < h) && (c < w)) {\n",
    "    out[r * w + c] = res;\n",
    "  }\n",
    "}\n",
    "\n",
    "torch::Tensor tiled_matmul(const torch::Tensor& m, const torch::Tensor& n) {\n",
    "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
    "    int h = m.size(0);\n",
    "    int w = n.size(1);\n",
    "    int k = m.size(1);\n",
    "    TORCH_CHECK(k==n.size(0), \"Size mismatch\");\n",
    "    //TORCH_CHECK((k % TILE_SIZE == 0) && (h % TILE_SIZE == 0) && (w % TILE_SIZE == 0), \"Padding not done\");\n",
    "    auto output = torch::empty({h, w}, m.options());\n",
    "\n",
    "    dim3 tpb(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
    "    tiled_matmul_kernel<<<blocks, tpb>>>(\n",
    "        output.data_ptr<float>(), m.data_ptr<float>(), n.data_ptr<float>(), h, w, k);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "cpp_src = \"\"\"\n",
    "torch::Tensor tiled_matmul(const torch::Tensor& m, const torch::Tensor& n);\n",
    "\"\"\"\n",
    "\n",
    "tiled_matmul_module = torch.utils.cpp_extension.load_inline(\n",
    "    \"test_ext_tiled_matmul\", cpp_src, cuda_src, \n",
    "    functions=['tiled_matmul'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b9ceaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707 µs ± 6.36 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tiled_matmul_module.tiled_matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e0f2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5776e-05, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.randn(500, 200, device=\"cuda\")\n",
    "bb = torch.randn(200, 1000, device=\"cuda\")\n",
    "\n",
    "\n",
    "(tiled_matmul_module.tiled_matmul(aa, bb) - aa@bb).abs().max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc3b49",
   "metadata": {},
   "source": [
    "# Occupancy?\n",
    "\n",
    "- shared memory: 64k/2k -> 32\n",
    "- threads: 1536/256 -> 6\n",
    "\n",
    "$\\rightarrow$ we could afford larger tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a78320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659c02af",
   "metadata": {},
   "source": [
    "## Thread Linearization and division into warps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fcb085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...\n",
      "The input conditions for extension module test_thread_idx have changed. Bumping to version 4 and re-building as test_thread_idx_v4...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu121/test_thread_idx/build.ninja...\n",
      "/root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module test_thread_idx_v4...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/lib/ccache/g++-11 -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=test_thread_idx_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/TH -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/miniconda3/envs/llm/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py39_cu121/test_thread_idx/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -ccbin /usr/lib/ccache/gcc-11 -DTORCH_EXTENSION_NAME=test_thread_idx_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/TH -isystem /root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/miniconda3/envs/llm/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' --ptxas-options=-v -std=c++17 -c /root/.cache/torch_extensions/py39_cu121/test_thread_idx/cuda.cu -o cuda.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z30thread_idx_of_neighbors_kernelPi' for 'sm_75'\n",
      "ptxas info    : Function properties for _Z30thread_idx_of_neighbors_kernelPi\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 60 registers, 360 bytes cmem[0]\n",
      "[3/3] /usr/lib/ccache/g++-11 main.o cuda.cuda.o -shared -L/root/miniconda3/envs/llm/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o test_thread_idx_v4.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module test_thread_idx_v4...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [2, 0, 0],\n",
       "        [3, 0, 0],\n",
       "        [4, 0, 0],\n",
       "        [5, 0, 0],\n",
       "        [6, 0, 0],\n",
       "        [7, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 1, 0],\n",
       "        [2, 1, 0],\n",
       "        [3, 1, 0],\n",
       "        [4, 1, 0],\n",
       "        [5, 1, 0],\n",
       "        [6, 1, 0],\n",
       "        [7, 1, 0],\n",
       "        [0, 2, 0],\n",
       "        [1, 2, 0],\n",
       "        [2, 2, 0],\n",
       "        [3, 2, 0],\n",
       "        [4, 2, 0],\n",
       "        [5, 2, 0],\n",
       "        [6, 2, 0],\n",
       "        [7, 2, 0],\n",
       "        [0, 3, 0],\n",
       "        [1, 3, 0],\n",
       "        [2, 3, 0],\n",
       "        [3, 3, 0],\n",
       "        [4, 3, 0],\n",
       "        [5, 3, 0],\n",
       "        [6, 3, 0],\n",
       "        [7, 3, 0]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_begin = r'''\n",
    "#include <torch/extension.h>\n",
    "#include <stdio.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "'''\n",
    "\n",
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void thread_idx_of_neighbors_kernel(int32_t* out){\n",
    "    // \n",
    "    int x = threadIdx.x;\n",
    "    int y = threadIdx.y;\n",
    "    int z = threadIdx.z;\n",
    "\n",
    "    for (int i = 0; i < 32; i++){\n",
    "        /* \n",
    "        __shfl_sync(mask, var, srcLane, width): 获取同一个 warp 第srcLane个线程的 x, y, z 变量值\n",
    "            mask: 一个用于同步的掩码，这里使用 0xffffffff 表示全同步\n",
    "        */\n",
    "        int other_x = __shfl_sync(0xffffffff, x, i);\n",
    "        int other_y = __shfl_sync(0xffffffff, y, i);\n",
    "        int other_z = __shfl_sync(0xffffffff, z, i);\n",
    "        out[z * 8*8*32*3 + y * 8*32*3 + x *32*3 + i*3] = other_x;\n",
    "        out[z * 8*8*32*3 + y * 8*32*3 + x *32*3 + i*3 + 1] = other_y;\n",
    "        out[z * 8*8*32*3 + y * 8*32*3 + x *32*3 + i*3 + 2] = other_z;\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor thread_idx_of_neighbors(){\n",
    "    // out[z,y,x]: 线程(x,y,z)所在warp内的32个\"邻居\"的索引\n",
    "    auto output = torch::empty({8,8,8,32,3}, torch::TensorOptions().device(torch::kCUDA).dtype(torch::kInt));\n",
    "\n",
    "    dim3 block(8, 8, 8);\n",
    "    dim3 grid(1);\n",
    "    thread_idx_of_neighbors_kernel<<<grid, block>>>(output.data_ptr<int32_t>());\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "\n",
    "cpp_src = \"\"\"\n",
    "    torch::Tensor thread_idx_of_neighbors();\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['CXX'] = '/usr/lib/ccache/g++-11'\n",
    "os.environ['CC'] = '/usr/lib/ccache/gcc-11'\n",
    "\n",
    "thread_idx_of_neighbors_module = torch.utils.cpp_extension.load_inline(\n",
    "    \"test_thread_idx\", cpp_src, cuda_src, \n",
    "    functions = [\"thread_idx_of_neighbors\"], extra_cuda_cflags = [\"--ptxas-options=-v\"], verbose=True\n",
    ")\n",
    "\n",
    "t = thread_idx_of_neighbors_module.thread_idx_of_neighbors()\n",
    "t[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7d6176b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 2],\n",
       "        [1, 0, 2],\n",
       "        [2, 0, 2],\n",
       "        [3, 0, 2],\n",
       "        [4, 0, 2],\n",
       "        [5, 0, 2],\n",
       "        [6, 0, 2],\n",
       "        [7, 0, 2],\n",
       "        [0, 1, 2],\n",
       "        [1, 1, 2],\n",
       "        [2, 1, 2],\n",
       "        [3, 1, 2],\n",
       "        [4, 1, 2],\n",
       "        [5, 1, 2],\n",
       "        [6, 1, 2],\n",
       "        [7, 1, 2],\n",
       "        [0, 2, 2],\n",
       "        [1, 2, 2],\n",
       "        [2, 2, 2],\n",
       "        [3, 2, 2],\n",
       "        [4, 2, 2],\n",
       "        [5, 2, 2],\n",
       "        [6, 2, 2],\n",
       "        [7, 2, 2],\n",
       "        [0, 3, 2],\n",
       "        [1, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [3, 3, 2],\n",
       "        [4, 3, 2],\n",
       "        [5, 3, 2],\n",
       "        [6, 3, 2],\n",
       "        [7, 3, 2]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[2,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5450f37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [2, 0, 0],\n",
       "        [3, 0, 0],\n",
       "        [4, 0, 0],\n",
       "        [5, 0, 0],\n",
       "        [6, 0, 0],\n",
       "        [7, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 1, 0],\n",
       "        [2, 1, 0],\n",
       "        [3, 1, 0],\n",
       "        [4, 1, 0],\n",
       "        [5, 1, 0],\n",
       "        [6, 1, 0],\n",
       "        [7, 1, 0],\n",
       "        [0, 2, 0],\n",
       "        [1, 2, 0],\n",
       "        [2, 2, 0],\n",
       "        [3, 2, 0],\n",
       "        [4, 2, 0],\n",
       "        [5, 2, 0],\n",
       "        [6, 2, 0],\n",
       "        [7, 2, 0],\n",
       "        [0, 3, 0],\n",
       "        [1, 3, 0],\n",
       "        [2, 3, 0],\n",
       "        [3, 3, 0],\n",
       "        [4, 3, 0],\n",
       "        [5, 3, 0],\n",
       "        [6, 3, 0],\n",
       "        [7, 3, 0]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0,0,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de5187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
